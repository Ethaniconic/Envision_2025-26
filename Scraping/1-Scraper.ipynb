{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8630a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üìÇ Loaded existing dataset with 500 rows ---\n",
      "--- üïµÔ∏è STARTING GREY MARKET BOOST (Target: 300 new rows) ---\n",
      "\n",
      "Processing High-Risk Query: first copy watches men...\n",
      "  - Offset 0: Found 1 items (1 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 7 items (2 confirmed Grey Market so far)\n",
      "  - Offset 40: Found 4 items (5 confirmed Grey Market so far)\n",
      "  - Offset 60: Found 7 items (9 confirmed Grey Market so far)\n",
      "  - Offset 80: Found 3 items (11 confirmed Grey Market so far)\n",
      "  - Offset 100: Found 5 items (14 confirmed Grey Market so far)\n",
      "  - Offset 120: Found 4 items (16 confirmed Grey Market so far)\n",
      "  - Offset 140: Found 13 items (25 confirmed Grey Market so far)\n",
      "  - Offset 160: Found 7 items (31 confirmed Grey Market so far)\n",
      "  - Offset 180: Found 9 items (35 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: 7a quality watch...\n",
      "  - Offset 0: Found 10 items (35 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: replica shoes for men...\n",
      "  - Offset 0: Found 16 items (35 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 15 items (35 confirmed Grey Market so far)\n",
      "  - Offset 40: Found 15 items (35 confirmed Grey Market so far)\n",
      "  - Offset 60: Found 13 items (35 confirmed Grey Market so far)\n",
      "  - Offset 80: Found 11 items (35 confirmed Grey Market so far)\n",
      "  - Offset 100: Found 11 items (35 confirmed Grey Market so far)\n",
      "  - Offset 120: Found 14 items (35 confirmed Grey Market so far)\n",
      "  - Offset 140: Found 9 items (35 confirmed Grey Market so far)\n",
      "  - Offset 160: Found 14 items (35 confirmed Grey Market so far)\n",
      "  - Offset 180: Found 18 items (35 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: master copy watch...\n",
      "  - Offset 0: Found 18 items (48 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 12 items (57 confirmed Grey Market so far)\n",
      "  - Offset 40: Found 2 items (58 confirmed Grey Market so far)\n",
      "  - Offset 60: Found 3 items (60 confirmed Grey Market so far)\n",
      "  - Offset 80: Found 5 items (64 confirmed Grey Market so far)\n",
      "  - Offset 100: Found 3 items (66 confirmed Grey Market so far)\n",
      "  - Offset 120: Found 5 items (69 confirmed Grey Market so far)\n",
      "  - Offset 140: Found 8 items (72 confirmed Grey Market so far)\n",
      "  - Offset 160: Found 7 items (77 confirmed Grey Market so far)\n",
      "  - Offset 180: Found 4 items (77 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: imported wireless headphones...\n",
      "  - Offset 0: Found 1 items (77 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 3 items (77 confirmed Grey Market so far)\n",
      "  - Offset 40: Found 1 items (77 confirmed Grey Market so far)\n",
      "  - Offset 60: Found 11 items (77 confirmed Grey Market so far)\n",
      "  - Offset 80: Found 14 items (77 confirmed Grey Market so far)\n",
      "  - Offset 100: Found 11 items (77 confirmed Grey Market so far)\n",
      "  - Offset 120: Found 20 items (77 confirmed Grey Market so far)\n",
      "  - Offset 140: Found 12 items (77 confirmed Grey Market so far)\n",
      "  - Offset 160: Found 10 items (77 confirmed Grey Market so far)\n",
      "  - Offset 180: Found 19 items (77 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: clone iphone...\n",
      "  - Offset 0: Found 20 items (77 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 15 items (77 confirmed Grey Market so far)\n",
      "  - Offset 40: Found 17 items (77 confirmed Grey Market so far)\n",
      "  - Offset 60: Found 19 items (77 confirmed Grey Market so far)\n",
      "  - Offset 80: Found 19 items (77 confirmed Grey Market so far)\n",
      "  - Offset 100: Found 19 items (77 confirmed Grey Market so far)\n",
      "  - Offset 120: Found 18 items (77 confirmed Grey Market so far)\n",
      "  - Offset 140: Found 19 items (77 confirmed Grey Market so far)\n",
      "  - Offset 160: Found 20 items (77 confirmed Grey Market so far)\n",
      "  - Offset 180: Found 20 items (77 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: first copy airpods...\n",
      "  - Offset 0: Found 0 items (77 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: replica sunglasses...\n",
      "  - Offset 0: Found 20 items (77 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 18 items (77 confirmed Grey Market so far)\n",
      "  - Offset 40: Found 19 items (77 confirmed Grey Market so far)\n",
      "  - Offset 60: Found 16 items (77 confirmed Grey Market so far)\n",
      "  - Offset 80: Found 18 items (77 confirmed Grey Market so far)\n",
      "  - Offset 100: Found 14 items (77 confirmed Grey Market so far)\n",
      "  - Offset 120: Found 12 items (77 confirmed Grey Market so far)\n",
      "  - Offset 140: Found 14 items (77 confirmed Grey Market so far)\n",
      "  - Offset 160: Found 12 items (77 confirmed Grey Market so far)\n",
      "  - Offset 180: Found 15 items (77 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: first copy shoes...\n",
      "  - Offset 0: Found 19 items (77 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 6 items (77 confirmed Grey Market so far)\n",
      "\n",
      "Processing High-Risk Query: imported smart watch...\n",
      "  - Offset 0: Found 20 items (83 confirmed Grey Market so far)\n",
      "  - Offset 20: Found 19 items (91 confirmed Grey Market so far)\n",
      "  - Offset 40: Found 14 items (95 confirmed Grey Market so far)\n",
      "  - Offset 60: Found 15 items (101 confirmed Grey Market so far)\n",
      "  - Offset 80: Found 12 items (105 confirmed Grey Market so far)\n",
      "  - Offset 100: Found 13 items (106 confirmed Grey Market so far)\n",
      "  - Offset 120: Found 16 items (111 confirmed Grey Market so far)\n",
      "  - Offset 140: Found 15 items (114 confirmed Grey Market so far)\n",
      "  - Offset 160: Found 16 items (122 confirmed Grey Market so far)\n",
      "  - Offset 180: Found 19 items (125 confirmed Grey Market so far)\n",
      "\n",
      "‚úÖ SUCCESS! Added 903 new rows.\n",
      "üìä Total Dataset Size: 1403\n",
      "New Class Distribution:\n",
      "Is_Grey_Market\n",
      "0    1242\n",
      "1     161\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# 1. SETUP & LOAD EXISTING DATA\n",
    "filename = \"snapdeal_mass_dataset.csv\"\n",
    "\n",
    "if os.path.exists(filename):\n",
    "    df_existing = pd.read_csv(filename)\n",
    "    print(f\"--- üìÇ Loaded existing dataset with {len(df_existing)} rows ---\")\n",
    "    existing_links = set(df_existing['Product_Link'].tolist())\n",
    "else:\n",
    "    print(\"--- ‚ö†Ô∏è No existing dataset found. Starting fresh. ---\")\n",
    "    df_existing = pd.DataFrame()\n",
    "    existing_links = set()\n",
    "\n",
    "# 2. DEFINE \"HIGH RISK\" KEYWORDS\n",
    "# These keywords are specifically chosen to find Grey Market/Fake items\n",
    "grey_market_keywords = [\n",
    "    \"first copy watches men\",\n",
    "    \"7a quality watch\",\n",
    "    \"replica shoes for men\",\n",
    "    \"master copy watch\",\n",
    "    \"imported wireless headphones\",\n",
    "    \"clone iphone\",\n",
    "    \"first copy airpods\",\n",
    "    \"replica sunglasses\",\n",
    "    \"first copy shoes\",\n",
    "    \"imported smart watch\"\n",
    "]\n",
    "\n",
    "base_url = \"https://www.snapdeal.com/search?keyword={}&sort=rlvncy&start={}\"\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "}\n",
    "\n",
    "new_data = []\n",
    "target_grey_count = 300\n",
    "collected_grey_count = 0\n",
    "\n",
    "print(f\"--- üïµÔ∏è STARTING GREY MARKET BOOST (Target: {target_grey_count} new rows) ---\")\n",
    "\n",
    "for query in grey_market_keywords:\n",
    "    if collected_grey_count >= target_grey_count:\n",
    "        break\n",
    "\n",
    "    print(f\"\\nProcessing High-Risk Query: {query}...\")\n",
    "    \n",
    "    start_offset = 0\n",
    "    # Search deeper (up to 10 pages) for these specific fake items\n",
    "    while start_offset < 200: \n",
    "        url = base_url.format(query.replace(\" \", \"%20\"), start_offset)\n",
    "        \n",
    "        try:\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            products = soup.find_all('div', class_='product-tuple-listing')\n",
    "            \n",
    "            if not products:\n",
    "                break\n",
    "            \n",
    "            batch_count = 0\n",
    "            for item in products:\n",
    "                if collected_grey_count >= target_grey_count:\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    # A. Scrape Details\n",
    "                    title_tag = item.find('p', class_='product-title')\n",
    "                    title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "                    \n",
    "                    link_tag = item.find('a', class_='dp-widget-link')\n",
    "                    link = link_tag['href'] if link_tag else \"\"\n",
    "                    \n",
    "                    # SKIP IF DUPLICATE (Already in existing file or current batch)\n",
    "                    if link in existing_links:\n",
    "                        continue\n",
    "\n",
    "                    price_tag = item.find('span', class_='lfloat product-price')\n",
    "                    price_str = price_tag.get_text(strip=True).replace('Rs.', '').replace(',', '').strip() if price_tag else \"0\"\n",
    "                    price = float(price_str) if price_str.replace('.','').isdigit() else 0\n",
    "                    \n",
    "                    mrp_tag = item.find('span', class_='lfloat product-desc-price strike')\n",
    "                    mrp_str = mrp_tag.get_text(strip=True).replace('Rs.', '').replace(',', '').strip() if mrp_tag else str(price)\n",
    "                    mrp = float(mrp_str) if mrp_str.replace('.','').isdigit() else price\n",
    "\n",
    "                    rating_tag = item.find('p', class_='product-rating-count')\n",
    "                    reviews = rating_tag.get_text(strip=True).replace('(', '').replace(')', '') if rating_tag else \"0\"\n",
    "\n",
    "                    # B. Check \"Is_Grey_Market\" Logic IMMEDIATELY\n",
    "                    # We only want to keep this item if it helps balance the dataset\n",
    "                    score = 0\n",
    "                    text = title.lower()\n",
    "                    \n",
    "                    # Strict Grey Market Rules\n",
    "                    if any(x in text for x in ['copy', 'replica', '7a', 'clone', 'master', 'import']):\n",
    "                        score += 50\n",
    "                    if \"watch\" in text and price < 500: # Cheap \"Rolex\" copies\n",
    "                        score += 20\n",
    "                    if mrp > 0 and (1 - (price/mrp)) > 0.75: # >75% discount\n",
    "                        score += 20\n",
    "                    \n",
    "                    is_grey = 1 if score >= 30 else 0\n",
    "\n",
    "                    # C. Save (Append)\n",
    "                    # We accept ALL items from these keywords, but we expect mostly Grey ones.\n",
    "                    new_data.append({\n",
    "                        \"Search_Term\": query,\n",
    "                        \"Product_Title\": title,\n",
    "                        \"Selling_Price\": price,\n",
    "                        \"MRP\": mrp,\n",
    "                        \"Discount_Pct\": round((1 - (price/mrp))*100, 1) if mrp > 0 else 0,\n",
    "                        \"Review_Count\": reviews,\n",
    "                        \"Product_Link\": link,\n",
    "                        \"Is_Grey_Market\": is_grey\n",
    "                    })\n",
    "                    \n",
    "                    existing_links.add(link)\n",
    "                    if is_grey == 1:\n",
    "                        collected_grey_count += 1\n",
    "                    batch_count += 1\n",
    "                        \n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  - Offset {start_offset}: Found {batch_count} items ({collected_grey_count} confirmed Grey Market so far)\")\n",
    "            start_offset += 20\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            break\n",
    "\n",
    "# 3. MERGE AND SAVE\n",
    "if new_data:\n",
    "    df_new = pd.DataFrame(new_data)\n",
    "    \n",
    "    # Combine with existing\n",
    "    df_final = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "    \n",
    "    # Save back to same file\n",
    "    df_final.to_csv(filename, index=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ SUCCESS! Added {len(df_new)} new rows.\")\n",
    "    print(f\"üìä Total Dataset Size: {len(df_final)}\")\n",
    "    print(\"New Class Distribution:\")\n",
    "    print(df_final['Is_Grey_Market'].value_counts())\n",
    "else:\n",
    "    print(\"‚ùå No new data found. Try adding more keywords.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
