{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8630a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- üïµÔ∏è STARTING MASS SCRAPE (Target: 100 rows per product) ---\n",
      "\n",
      "Processing: first copy watches...\n",
      "  - Offset 0: Grabbed 20 items. Total: 20/100\n",
      "  - Offset 20: Grabbed 16 items. Total: 36/100\n",
      "  - Offset 40: Grabbed 19 items. Total: 55/100\n",
      "  - Offset 60: Grabbed 17 items. Total: 72/100\n",
      "  - Offset 80: Grabbed 15 items. Total: 87/100\n",
      "  - Offset 100: Grabbed 13 items. Total: 100/100\n",
      "\n",
      "Processing: replica shoes...\n",
      "  - Offset 0: Grabbed 20 items. Total: 20/100\n",
      "  - Offset 20: Grabbed 17 items. Total: 37/100\n",
      "  - Offset 40: Grabbed 11 items. Total: 48/100\n",
      "  - Offset 60: Grabbed 17 items. Total: 65/100\n",
      "  - Offset 80: Grabbed 19 items. Total: 84/100\n",
      "  - Offset 100: Grabbed 16 items. Total: 100/100\n",
      "\n",
      "Processing: imported headphones...\n",
      "  - Offset 0: Grabbed 20 items. Total: 20/100\n",
      "  - Offset 20: Grabbed 18 items. Total: 38/100\n",
      "  - Offset 40: Grabbed 13 items. Total: 51/100\n",
      "  - Offset 60: Grabbed 17 items. Total: 68/100\n",
      "  - Offset 80: Grabbed 15 items. Total: 83/100\n",
      "  - Offset 100: Grabbed 10 items. Total: 93/100\n",
      "  - Offset 120: Grabbed 7 items. Total: 100/100\n",
      "\n",
      "Processing: analog watch men...\n",
      "  - Offset 0: Grabbed 7 items. Total: 7/100\n",
      "  - Offset 20: Grabbed 9 items. Total: 16/100\n",
      "  - Offset 40: Grabbed 10 items. Total: 26/100\n",
      "  - Offset 60: Grabbed 7 items. Total: 33/100\n",
      "  - Offset 80: Grabbed 9 items. Total: 42/100\n",
      "  - Offset 100: Grabbed 11 items. Total: 53/100\n",
      "  - Offset 120: Grabbed 11 items. Total: 64/100\n",
      "  - Offset 140: Grabbed 15 items. Total: 79/100\n",
      "  - Offset 160: Grabbed 13 items. Total: 92/100\n",
      "  - Offset 180: Grabbed 8 items. Total: 100/100\n",
      "\n",
      "Processing: running shoes...\n",
      "  - Offset 0: Grabbed 20 items. Total: 20/100\n",
      "  - Offset 20: Grabbed 17 items. Total: 37/100\n",
      "  - Offset 40: Grabbed 16 items. Total: 53/100\n",
      "  - Offset 60: Grabbed 19 items. Total: 72/100\n",
      "  - Offset 80: Grabbed 12 items. Total: 84/100\n",
      "  - Offset 100: Grabbed 14 items. Total: 98/100\n",
      "  - Offset 120: Grabbed 2 items. Total: 100/100\n",
      "\n",
      "‚úÖ SCRAPING COMPLETE! Total Rows: 500\n",
      "üìÅ Data saved to: snapdeal_mass_dataset.csv\n",
      "\n",
      "Class Distribution:\n",
      "Is_Grey_Market\n",
      "0    464\n",
      "1     36\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "base_url = \"https://www.snapdeal.com/search?keyword={}&sort=rlvncy&start={}\"\n",
    "\n",
    "keywords = [\n",
    "    \"first copy watches\",\n",
    "    \"replica shoes\",\n",
    "    \"imported headphones\",\n",
    "    \"analog watch men\",\n",
    "    \"running shoes\"\n",
    "]\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\"\n",
    "}\n",
    "\n",
    "all_data = []\n",
    "\n",
    "print(\"--- üïµÔ∏è STARTING MASS SCRAPE (Target: 100 rows per product) ---\")\n",
    "\n",
    "for query in keywords:\n",
    "    print(f\"\\nProcessing: {query}...\")\n",
    "    \n",
    "    items_collected = 0\n",
    "    start_offset = 0\n",
    "    \n",
    "    while items_collected < 100:\n",
    "        url = base_url.format(query.replace(\" \", \"%20\"), start_offset)\n",
    "        \n",
    "        try:\n",
    "            time.sleep(random.uniform(2, 4))\n",
    "            \n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"  - Blocked or Error at offset {start_offset}. Stopping this keyword.\")\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            products = soup.find_all('div', class_='product-tuple-listing')\n",
    "            \n",
    "            if not products:\n",
    "                print(\"  - No more items found. Moving to next keyword.\")\n",
    "                break\n",
    "                \n",
    "            batch_count = 0\n",
    "            for item in products:\n",
    "                if items_collected >= 100:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    title_tag = item.find('p', class_='product-title')\n",
    "                    title = title_tag.get_text(strip=True) if title_tag else \"N/A\"\n",
    "                    \n",
    "                    price_tag = item.find('span', class_='lfloat product-price')\n",
    "                    price_str = price_tag.get_text(strip=True).replace('Rs.', '').replace(',', '').strip() if price_tag else \"0\"\n",
    "                    price = float(price_str) if price_str.replace('.','').isdigit() else 0\n",
    "                    \n",
    "                    mrp_tag = item.find('span', class_='lfloat product-desc-price strike')\n",
    "                    mrp_str = mrp_tag.get_text(strip=True).replace('Rs.', '').replace(',', '').strip() if mrp_tag else str(price)\n",
    "                    mrp = float(mrp_str) if mrp_str.replace('.','').isdigit() else price\n",
    "                    \n",
    "                    rating_tag = item.find('p', class_='product-rating-count')\n",
    "                    reviews = rating_tag.get_text(strip=True).replace('(', '').replace(')', '') if rating_tag else \"0\"\n",
    "                    \n",
    "                    link_tag = item.find('a', class_='dp-widget-link')\n",
    "                    link = link_tag['href'] if link_tag else \"\"\n",
    "\n",
    "                    if not any(d['Product_Link'] == link for d in all_data):\n",
    "                        all_data.append({\n",
    "                            \"Search_Term\": query,\n",
    "                            \"Product_Title\": title,\n",
    "                            \"Selling_Price\": price,\n",
    "                            \"MRP\": mrp,\n",
    "                            \"Discount_Pct\": round((1 - (price/mrp))*100, 1) if mrp > 0 else 0,\n",
    "                            \"Review_Count\": reviews,\n",
    "                            \"Product_Link\": link\n",
    "                        })\n",
    "                        items_collected += 1\n",
    "                        batch_count += 1\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    continue\n",
    "            \n",
    "            print(f\"  - Offset {start_offset}: Grabbed {batch_count} items. Total: {items_collected}/100\")\n",
    "            \n",
    "            start_offset += 20\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  - Critical Error: {e}\")\n",
    "            break\n",
    "\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "if not df.empty:\n",
    "    print(f\"\\n‚úÖ SCRAPING COMPLETE! Total Rows: {len(df)}\")\n",
    "    \n",
    "    def check_suspicious(row):\n",
    "        score = 0\n",
    "        text = row['Product_Title'].lower()\n",
    "        \n",
    "        if any(x in text for x in ['copy', 'replica', 'compatible with', '7a', 'import']):\n",
    "            score += 50\n",
    "            \n",
    "        if \"watch\" in text and row['Selling_Price'] < 400:\n",
    "            score += 20\n",
    "            \n",
    "        if row['Discount_Pct'] > 80:\n",
    "            score += 20\n",
    "            \n",
    "        return 1 if score >= 30 else 0\n",
    "\n",
    "    df['Is_Grey_Market'] = df.apply(check_suspicious, axis=1)\n",
    "    \n",
    "    filename = \"snapdeal_mass_dataset.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"üìÅ Data saved to: {filename}\")\n",
    "    print(\"\\nClass Distribution:\")\n",
    "    print(df['Is_Grey_Market'].value_counts())\n",
    "else:\n",
    "    print(\"‚ùå Failed to collect data. Check your connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521bf955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
